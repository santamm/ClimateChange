{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "71a141488ec9d223c483fcb8a68f946528cc1f33"
   },
   "source": [
    "# Welcome to the AIinAfrica Hackaton!!\n",
    "\n",
    "## Climate Change: Earth Surface Temperature Data contest\n",
    "### Introduction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0156f6f80ebb066574b14726e1cfeaaa1da7d35d"
   },
   "source": [
    "In this notebook we explore a dataset of monthly temperatures across the globe dated from 1750 to 2015. We will investigate if temperatures have been rising and we will compare Machine Learning models to predict accurately future temperatures in countries an cities of choice.     \n",
    "    \n",
    "The dataset we use is the [World Bank Climate Change Data from](https://data.worldbank.org/topic/climate-change) and can be downloaded from this [Kaggle page](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "It contains 3 csv files:\n",
    "\n",
    "`GlobalLandTemperaturesByCity.csv`\n",
    "\n",
    "`GlobalLandTemperaturesByCountry.csv`\n",
    "\n",
    "`GlobalTemperatures.csv`\n",
    "\n",
    "In the first part we analyze the dataset to verify some statistical properties, like the stationarity. Stationarity means that the statistical properties of a process generating a time series do not change over time. It does not mean that the series does not change over time, just that the way it changes does not itself change over time. \n",
    "\n",
    "Then we check is the data is seasonal, as we expect from land and sea temperatures.\n",
    "\n",
    "Finally, we use a Natural diasaster data  taken from https://ourworldindata.org/natural-disasters (data published by EMDAT (2019): OFDA/CRED International Disaster Database, Université catholique de Louvain – Brussels – Belgium), that includes the following files:\n",
    "\n",
    "`number-of-natural-disaster-events.csv`\n",
    "\n",
    "`natural-disasters-by-type.csv`\n",
    "\n",
    "`economic-damage-from-natural-disasters.csv`\n",
    "\n",
    "We will show how there has been an increase in natural disaster occurrences in the last 50 years and how they are correlated to climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f5be0610c29b7074159aeb14f1567266b1bac6cb"
   },
   "source": [
    "#### Importing required modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4042411571dc8f992288ba2eedb7b9094e27f10d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import datetime as dt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "969b95d032a6550625c1b69b0db71a1a6d965e4f"
   },
   "source": [
    "#### **Loading the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"data/GlobalTemperatures.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")\n",
    "with zipfile.ZipFile(\"data/GlobalLandTemperaturesByCountry.csv.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"data\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_temperatures = pd.read_csv('data/GlobalTemperatures.csv')\n",
    "country_temperatures = pd.read_csv('data/GlobalLandTemperaturesByCountry.csv')\n",
    "SA = pd.read_csv('data/SATemperaturesbyCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5388e760249748ab247cec741fdccb76a0b4979c"
   },
   "outputs": [],
   "source": [
    "#city_temperatures = pd.read_csv('data/GlobalLandTemperaturesByCity.csv')\n",
    "#SA = city_temperatures[city_temperatures.Country == 'South Africa']\n",
    "#SA.set_index('dt', inplace=True)\n",
    "#SA.to_csv('data/SATemperaturesbyCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "502ec3f6ea42c11441fe4fe4a182f20e46805c7f"
   },
   "outputs": [],
   "source": [
    "#SA.to_csv('data/SATemperaturesbyCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b70c0370ebca58a379f846aaf2d47ed1668e6ef3"
   },
   "outputs": [],
   "source": [
    "SA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_temperatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37e2c3cdd4c7acfd2c38d63a9f7a65ab9b9fa237"
   },
   "outputs": [],
   "source": [
    "country_temperatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08f106b74a64d7bdf046cdb92a1d64ac63938220"
   },
   "outputs": [],
   "source": [
    "global_temperatures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29cbe778a2e2b26445010ca631be4f5f99722b7b"
   },
   "outputs": [],
   "source": [
    "global_temperatures.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dt from string to datetime\n",
    "global_temperatures['dt'] = pd.to_datetime(global_temperatures.dt)\n",
    "global_temperatures['year'] = global_temperatures['dt'].map(lambda x:x.year)\n",
    "global_temperatures['month'] = global_temperatures['dt'].map(lambda x:x.month)\n",
    "\n",
    "country_temperatures['dt'] = pd.to_datetime(country_temperatures.dt)\n",
    "country_temperatures['year'] = country_temperatures['dt'].map(lambda x:x.year)\n",
    "country_temperatures['month'] = country_temperatures['dt'].map(lambda x:x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dt from string to datetime\n",
    "SA['dt'] = pd.to_datetime(SA.dt)\n",
    "SA['year'] = SA['dt'].map(lambda x:x.year)\n",
    "SA['month'] = SA['dt'].map(lambda x:x.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ab32500cbfe5d10204dac9e09c99c72be1fe238"
   },
   "source": [
    "# Is The Global Temperature Really Rising??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take the avg temperature per year\n",
    "global_mean=global_temperatures.groupby(['year'])['LandAverageTemperature'].mean().reset_index()\n",
    "global_mean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8b6dc52e6f5a8c19c97da537723c8d33404ac91"
   },
   "outputs": [],
   "source": [
    "# Visualize Plot\n",
    "trace=go.Scatter(\n",
    "    x=global_mean['year'],\n",
    "    y=global_mean['LandAverageTemperature'],\n",
    "    mode='lines',\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        text='Global Average Temperatures 1750-2015',\n",
    "        xref='paper',\n",
    "        x=0\n",
    "    ))\n",
    "\n",
    "data=[trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='line-mode')\n",
    "#title=\"Global Average Temperatures 1750-2015\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In climate science, temperature anomaly represents temperature pattern accurately than absolute temperature. It is a measure of the departure from baseline temperature. Basically, it indicates how much warmer or cooler it is than the baseline. The baseline used here is the average temperature over the 30 year period 1951-1980 (base period used by NASA).<br><br>The temperature anomaly values are stored in a new column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_mean.head()\n",
    "baseline_temp = global_mean.loc['1951':'1980'].mean()['LandAverageTemperature'] \n",
    "baseline_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the global baseline temperature\n",
    "baseline_temp = global_mean.loc['1951':'1980'].mean()['LandAverageTemperature'] \n",
    "\n",
    "# Create the temperature anomaly column\n",
    "global_mean['TemperatureAnomaly'] = global_mean['LandAverageTemperature'] - baseline_temp\n",
    "\n",
    "global_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Plot with Temperature Anomalies starting from 1900, as measurements were more accurate \n",
    "trace=go.Scatter(\n",
    "    x=global_mean[global_mean['year'] > 1900].year,\n",
    "    y=global_mean[global_mean['year'] > 1900].TemperatureAnomaly,\n",
    "    mode='lines',\n",
    "    )\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        text='Global Temperature Anomalies 1900-2015',\n",
    "        xref='paper',\n",
    "        x=0\n",
    "    ))\n",
    "data=[trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='line-mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the warming of Earth. The warming is more pronounced from about 1920 onwards. In 2015, the Earth is 0.76 degree Celsius warmer than the normal. The result above confirms the scientific consensus that the Earth is warming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dicky Fuller Stationarity Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dicky Fuller Stationarity Test (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1911068)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series, title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4], index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(out.to_string())          # .to_string() removes the line \"dtype: float64\"\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")\n",
    "        \n",
    "\n",
    "adf_test(global_mean.LandAverageTemperature, title='DFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Deconstruction:\n",
    "#### 1. Trend: Reflects long term progression of series\n",
    "#### 2. Seasonality: Reflects seasonal variation and checks if seasonal pattern exists in the time series\n",
    "#### 3. Residual: Shows irregular component (or \"noise\") at time t, which describes random, irregular influences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = global_temperatures[['dt','LandAverageTemperature']]\n",
    "rng = pd.date_range(start='1901-01-01', end='2015-12-01', freq = 'MS')\n",
    "\n",
    "data = ts[ts.dt.dt.year>1900].LandAverageTemperature.values\n",
    "series = pd.Series(data=data, index=rng)\n",
    "len(data), len(series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ETS Graph (Error Trend Seasonality)\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "result = seasonal_decompose(series, model='additive')\n",
    "plt.figure(figsize=(15,8))\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6103a6f07a870ed8e0ca3228a6dcf2bbd47648ad"
   },
   "outputs": [],
   "source": [
    "# There exists 12 months seasonality in the dataset\n",
    "result.seasonal.loc['2010-01-01':'2012-01-01'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can look at measurements in January and July"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_month = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\n",
    "#global_temperatures['month']=global_temperatures['month'].map(dict_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_month(month1,month2):\n",
    "    a = global_temperatures[global_temperatures['month'] == month1]\n",
    "    b = global_temperatures[global_temperatures['month'] == month2]\n",
    "    trace0 = go.Scatter(\n",
    "    x = a['year'],\n",
    "    y = a['LandAverageTemperature'],\n",
    "    mode = 'lines',\n",
    "    name = dict_month[month1]\n",
    "    )\n",
    "    \n",
    "    trace1 = go.Scatter(\n",
    "    x = b['year'],\n",
    "    y = b['LandAverageTemperature'],\n",
    "    mode = 'lines',\n",
    "    name = dict_month[month2]\n",
    "    )\n",
    "    layout = go.Layout(\n",
    "    title=go.layout.Title(\n",
    "        text=\"Global Average Temperatures in {} and {} between 1900-2015\".format(dict_month[month1], dict_month[month2]),\n",
    "        xref='paper',\n",
    "        x=0\n",
    "    ))\n",
    "    data = [trace0,trace1]\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    py.iplot(fig, filename='line-mode')\n",
    "    \n",
    "plot_month(1,7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like July temperatures have always been higher. What if you split the dataset between the northern and southern emisphere and draw this graph on the two datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove the duplicated countries and the continents (in the analysis, we don't consider the presence of \n",
    "# colonies at this the countries) and countries for which no information about the temperature\n",
    "\n",
    "country_temp = country_temperatures[~country_temperatures['Country'].isin(\n",
    "    ['Denmark', 'France', 'Europe', 'Netherlands','United Kingdom', 'Africa', 'Europe', 'Asia', 'South America', 'North America'])]\n",
    "\n",
    "country_temp = country_temp.replace(\n",
    "   ['Denmark (Europe)', 'France (Europe)', 'Netherlands (Europe)', 'United Kingdom (Europe)'],\n",
    "   ['Denmark', 'France', 'Netherlands', 'United Kingdom'])\n",
    "\n",
    "#Let's average temperature for each country\n",
    "countries = np.unique(country_temp['Country'])\n",
    "mean_temp = []\n",
    "for country in countries:\n",
    "    mean_temp.append(country_temp[country_temp['Country'] == country]['AverageTemperature'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        #autocolorscale = True,\n",
    "        #colorscale = 'RdYlGn',\n",
    "        #reversescale = True,\n",
    "        showscale = True,\n",
    "        locations = countries,\n",
    "        z = mean_temp,\n",
    "        locationmode = 'country names', \n",
    "        marker = dict(\n",
    "            line = dict(color = 'rgb(200,200,200)', width = 0.5)),\n",
    "            colorbar = dict(autotick = True, tickprefix = '', \n",
    "            title = 'Temperature')\n",
    "            )\n",
    "       ]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Average Temperature By Country',\n",
    "    geo = dict(\n",
    "        showframe = True,\n",
    "        showocean = True,\n",
    "        oceancolor = 'rgb(0,255,255)',\n",
    "        projection = dict(\n",
    "        type = 'Mercator',\n",
    "            \n",
    "        ),\n",
    "            ),\n",
    "        )\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, validate=False, filename='worldmap2010')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the chart of annual temperature changes in certain continents (we take into consideration one country per continent and mark Greenland as the coldest place on Earth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = ['Africa', 'Europe', 'North America', 'South America', 'Australia', 'Asia']\n",
    "continents_temps = country_temperatures[country_temperatures.Country.isin(continents)]\n",
    "continents_temps['dt'] = pd.to_datetime(continents_temps.dt)\n",
    "continents_temps['year'] = continents_temps['dt'].map(lambda x:x.year)\n",
    "continents_temps['month'] = continents_temps['dt'].map(lambda x:x.month)\n",
    "\n",
    "continents_temps = continents_temps[continents_temps.year > 1900].groupby(['Country', 'year'])['AverageTemperature'].mean().reset_index().sort_values(by=['Country', 'year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = []\n",
    "#colors = ['rgb(0, 255, 255)', 'rgb(255, 0, 255)', 'rgb(0, 0, 0)',\n",
    "#          'rgb(255, 0, 0)', 'rgb(0, 255, 0)', 'rgb(0, 0, 255)']\n",
    "colors = ['red', 'blue', 'black', 'green', 'pink', 'magenta']\n",
    "years = continents_temps.year.unique()\n",
    "for i, continent in enumerate(continents):\n",
    "    traces.append(go.Scatter(\n",
    "        x=years,\n",
    "        y=continents_temps[continents_temps.Country == continent].AverageTemperature.values,\n",
    "        mode='lines',\n",
    "        name=continent,\n",
    "        line=dict(color=colors[i]),\n",
    "    ))\n",
    "    \n",
    "    \n",
    "layout = go.Layout(\n",
    "    xaxis=dict(title='Year'),\n",
    "    yaxis=dict(title='Average Temperature, °C'),\n",
    "    title='Average Land Temperature on the Continents',)\n",
    "\n",
    "fig = go.Figure(data=traces, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Difference By Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_temps = country_temp[country_temp.year > 1900].groupby('Country')['AverageTemperature'].max()\n",
    "min_temps = country_temp[country_temp.year > 1900].groupby('Country')['AverageTemperature'].min()\n",
    "differences = (max_temps - min_temps).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot(y=differences.values[:50], x=differences.index[:50], palette='RdYlGn').set_title('Countries with Highest Difference between Max And Min Temperature')\n",
    "plt.xticks(rotation=90);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's have a look at tge world map with temperature differences\n",
    "data = [ dict(\n",
    "        type = 'choropleth',\n",
    "        autocolorscale = False,\n",
    "        colorscale = 'Viridis',\n",
    "        reversescale = True,\n",
    "        showscale = True,\n",
    "        locations = differences.index,\n",
    "        z = differences.values,\n",
    "        locationmode = 'country names',\n",
    "        text = differences.index,\n",
    "        marker = dict(\n",
    "            line = dict(color = 'rgb(200,200,200)', width = 0.5)),\n",
    "            colorbar = dict(autotick = True, tickprefix = '', \n",
    "            title = 'Temperature Difference')\n",
    "            )\n",
    "       ]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Temperature Difference By Country',\n",
    "    geo = dict(\n",
    "        showframe = True,\n",
    "        showocean = True,\n",
    "        oceancolor = 'rgb(0,255,255)',\n",
    "        projection = dict(\n",
    "        type = 'Mercator',\n",
    "            \n",
    "        ),\n",
    "            ),\n",
    "        )\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig, validate=False, filename='worldmap2010')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cac910a6620e9d1ea5f00d478a4261e8d8a502a8"
   },
   "source": [
    "# What about   South Africa ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83e0863ff2107f864458467aeaffbd2388e80db8"
   },
   "outputs": [],
   "source": [
    "#SA = city_temperatures[city_temperatures.Country == 'South Africa']\n",
    "# Consider name changes\n",
    "SA['City'].replace({'Pietersburg':'Polokwane'}, inplace=True)\n",
    "#SA['month'] = SA.month.map(dict_month)\n",
    "SA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_avg_temps = SA.groupby(['City'])['AverageTemperature'].mean().reset_index().sort_values(by='AverageTemperature',ascending=False)\n",
    "#sa_avg_temps\n",
    "plt.subplots(figsize=(8,30))\n",
    "sns.barplot(y='City',  x='AverageTemperature', data=sa_avg_temps, palette='RdYlGn').set_title('Hottest Cities in SA')\n",
    "plt.xlabel('Average Temperature');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_cities=SA[SA['City'].isin(['Johannesburg','Cape Town','Bloemfontein','Rustenburg', 'Nelspruit', 'Durban','Port Elizabeth','Pretoria','Polokwane','East London'])]\n",
    "\n",
    "heatmap=major_cities.groupby(['City','month'])['AverageTemperature'].mean().reset_index().sort_values(by='month')\n",
    "heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Heatmap(z=heatmap['AverageTemperature'],\n",
    "                   x=heatmap['month'].map(dict_month),\n",
    "                   y=heatmap['City'],\n",
    "                  colorscale='Viridis')\n",
    "data=[trace]\n",
    "layout = go.Layout(\n",
    "    title='Average Temperature Of Major SA Cities By Month',\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='labelled-heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "055cce50507f970af56fcc27677f782b9abc93b3",
    "collapsed": true
   },
   "source": [
    "# Sequence Data\n",
    "\n",
    "\n",
    "\n",
    "A sequence is any type of data that is in a particular order in which one thing follows another. \n",
    "Sequence data comes in many forms. Audio is a natural sequence. You can chop up an audio spectrogram into chunks and feed that into a machine learning model. Text is another form of sequences. You can break Text up into a sequence of characters or a sequence of words.\n",
    "\n",
    "The simplest machine learning problem involving a sequence is a one to one problem.\n",
    "<br>\n",
    "![One2one](one2one.png \"One 2 One\")\n",
    "\n",
    "\n",
    "\n",
    "In this case, we have one data input or tensor to the model and the model generates a prediction with the given input.\n",
    "\n",
    "We can extend this formulation to allow for the model to make use of the past values of the input and the output. Now the output of the model is now fed back to the model as a new input. The model now can generate a new output and we can continue like this indefinitely.\n",
    "\n",
    "![Many to One](many2one.png \"Many 2 One\")\n",
    "\n",
    "There are called Recurrent Neural Networks (RNNs).\n",
    "RNNs can retain state from one iteration to the next by using their own output as input for the next step.\n",
    "\n",
    "\n",
    "A Time Series is a sequence where the data is in order, with a fixed time-difference between occurrence of successive data points. \n",
    "A Recurrent Neural Netowrk is a particular type of neural mnetwork that is  good at processing sequence data for predictions. That's because they implement a concept called sequential memory.\n",
    "Sequential memory is a mechanism that makes it easier for the brain to recognize sequence patterns.\n",
    "\n",
    "\n",
    "Traditional neural networks have an input layer, one or more hidden layers, and an output layer.\n",
    "\n",
    "![Traditional Feed-Forward Neural Network](ffnn.png \"Traditional Feed-Forward Neural Network\")\n",
    "\n",
    "Now we want to get a feed-forward neural network to be able to use previous information to effect later results. So we add a loop in the neural network that can pass prior information to later stages.\n",
    "\n",
    "![RNN](rnn.gif \"RNN\")\n",
    "\n",
    "Recurrent Neural Networks suffer from short-term memory. If a sequence is long enough, they’ll have a hard time carrying information from earlier time steps to later ones. So for us it will be diifcult to predict temperatures in 2020 while remembering what happened in the previous years.\n",
    "\n",
    "To solve this problem Long Short Term Memory Networks (LSTM) were introduced. They have internal mechanisms called gates that can regulate the flow of information. These gates can learn which data in a sequence is important to keep or throw away. By doing that, it can pass relevant information down the long chain of sequences to make predictions. LSTM are capable or memorizing and carrying a \"state\". You can think of it as the “memory” of the network. So even information from the earlier time steps can make it’s way to later time steps, reducing the effects of short-term memory. As the cell state goes on its journey, information get’s added or removed to the cell state via gates.\n",
    "![LSTM](lstm.png \"LSTM\")\n",
    "\n",
    "\n",
    "If you want to know more, you can have a look at the followig articles: \n",
    "\n",
    "[Illustrated Guide to recurrent Neural Networks by Michael Nguyen](https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n",
    "\n",
    "[Illustrated Guide to LSTM’s and GRU’s: A step by step explanation by Michael Nguyen](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n",
    "\n",
    "[A Guide For Time Series Prediction Using Recurrent Neural Networks (LSTMs) by Neelabh Pant\n",
    "](https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f)\n",
    "\n",
    "[Using LSTMs to forecast time-series by Ravindra Kompella\n",
    "](https://towardsdatascience.com/using-lstms-to-forecast-time-series-4ab688386b1f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global_mean.LandAverageTemperature.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "082965e9896131dfef1b628081f1d581f3d8f3a8"
   },
   "source": [
    "### Divide the dataset into training and validation sets\n",
    "It is very important when you do time series to split train and test with respect to a certain date. So, you don’t want your test data to come before your training data.\n",
    "\n",
    "Setting training set from: January 1901 to December 2000\n",
    "\n",
    "Setting validation set from: January 2001 to December 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = global_temperatures[['dt','LandAverageTemperature']]\n",
    "rng = pd.date_range(start='1901-01-01', end='2015-12-01', freq = 'MS')\n",
    "\n",
    "data = ts[ts.dt.dt.year>1900].LandAverageTemperature.values\n",
    "series = pd.Series(data=data, index=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(data=data, index=rng)\n",
    "\n",
    "train, test = dataset.iloc[:-180], dataset.iloc[-180:]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing to do is normalize the dataset. You only need to fit and transform your training data and just transform your test data. The reason you do that is you don’t want to assume that you know the scale of your test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "91b31542d8572835f10e14f13b75a8f4e4cb2155"
   },
   "outputs": [],
   "source": [
    "# Scaling values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(train)\n",
    "\n",
    "scaled_train = sc.transform(train)\n",
    "scaled_test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (One to One)\n",
    "\n",
    "This is a simple regression model that will take one input and will spit out one output. This basically takes the temperature from the previous month and forecasts the temperature of the next month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X_train = scaled_train[:-1]\n",
    "y_train = scaled_train[1:]\n",
    "\n",
    "X_test = scaled_test[:-1]\n",
    "#predict the test set results: insert last temperature from training set as first one of test set\n",
    "X_test = np.insert(X_test, 0, X_train[-1][0]).reshape(-1,1)\n",
    "#y_test = scaled_test[1:]\n",
    "y_test = scaled_test\n",
    "\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test temperatures\n",
    "y_pred = regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics \n",
    "y_pred_inv_scaled = sc.inverse_transform(y_pred)\n",
    "actuals = sc.inverse_transform(y_test)\n",
    "\n",
    "print('MSE: %f'%mean_squared_error(actuals, y_pred_inv_scaled))\n",
    "print('RMSE: %f'%np.sqrt(mean_squared_error(actuals, y_pred_inv_scaled)))\n",
    "print('R-Squared: %f'%r2_score(actuals, y_pred_inv_scaled))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the metric we need to memorize to compare with other models is: `RMSE: 2.099682`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual values\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(actuals, label=\"Actuals temps\")\n",
    "plt.plot(y_pred_inv_scaled, label=\"Predicted\") \n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(np.arange(len(y_pred))[::6], pd.date_range(start='2001-01-01', end='2015-12-01', freq = '6MS').strftime(' %b %Y'), rotation=\"vertical\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far it looks that the model fits properly to the training dta and performs well on tha validation data too. But what about generalizing to dates in the future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_y_pred = y_pred.copy()\n",
    "last = ext_y_pred[-1].reshape(-1,1) # This is the value predicted for 2016\n",
    "for i in range(24):\n",
    "    next_month = regressor.predict(last)\n",
    "    ext_y_pred=np.append(ext_y_pred, next_month)\n",
    "\n",
    "y_pred_inv_scaled = sc.inverse_transform(ext_y_pred)\n",
    "actuals = sc.inverse_transform(y_test)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.plot(actuals, label=\"Actuals temps\")\n",
    "plt.plot(y_pred_inv_scaled, label=\"Predicted\")  \n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(np.arange(len(y_pred_inv_scaled))[::6], pd.date_range(start='2001-01-01', periods=len(y_pred_inv_scaled), \n",
    "                                                           freq = '6MS').strftime(' %b %Y'), rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is not good. It essentially is repeating the previous values and there is a slight shift. The model can't capture the seasonality of the data and therefore is not able to predict the future from the single previous value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to One Models\n",
    "\n",
    "In order to solve the problem above where the linear one-to-one model can't predict seasonality we will move now to a many-to-one model: we use a moving forward window of size 60 (5 years), which means we will use the first 60 data points as out input X to predict y1 — the 61st data point. Next, we will use the window between 1 to 61 data points as input X to predict y2 i.e., the 62nd data point and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting from training and test sequences\n",
    "scaled_train.shape, scaled_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1304d3dfdff105a11c3ceb865bd21184a6020ea"
   },
   "outputs": [],
   "source": [
    "# We build a dataframe with 61 columns where the 61st column is the label we want to predict\n",
    "window_size = 60\n",
    "df = pd.DataFrame(np.concatenate((scaled_train, scaled_test)))\n",
    "df_s = df.copy()\n",
    "for i in range(window_size):\n",
    "    df = pd.concat([df, df_s.shift(-(i+1))], axis = 1)\n",
    "    \n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 60 columns are the features, 61st column is the label\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67e1e1f080207b68e41e252aca890fbe8c431066"
   },
   "source": [
    "Now we can split our new dataframe into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will be left with 1140 data points to train our models and 180 to validate them\n",
    "train, test = df.iloc[:-180, :], df.iloc[-180:,:]\n",
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features from labels\n",
    "X_train = train.iloc[:,:-1]\n",
    "y_train = train.iloc[:,-1]\n",
    "X_test = test.iloc[:,:-1]\n",
    "y_test = test.iloc[:,-1]\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We try first with a simple Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on all the test data at once\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving temperatures data from the scaled ones\n",
    "\n",
    "y_pred_inv_scaled = sc.inverse_transform(y_pred)\n",
    "actuals = sc.inverse_transform(y_test)\n",
    "actuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE: %f'%mean_squared_error(actuals, y_pred_inv_scaled))\n",
    "print('RMSE: %f'%np.sqrt(mean_squared_error(actuals, y_pred_inv_scaled)))\n",
    "print('R-Squared: %f'%r2_score(actuals, y_pred_inv_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can already see that this model has a much better performance compared to the one-to-one linear regression \n",
    "whose RMSE was much higher at `2.099682`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Predictions\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.ylabel(\"Avg Temperature\")\n",
    "plt.plot(actuals, label=\"Actuals temps\")\n",
    "plt.plot(y_pred_inv_scaled, label=\"Predicted\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(np.arange(180)[::6], pd.date_range(start='2001-01-01', end='2015-12-01', freq = '6MS').strftime(' %b %Y'), rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above it looks that the model fits very well the training and validation data, but what about generalizing to new data? we want to predict now temperatures for the next two years: 2016 and 2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_y_pred = y_pred.copy()\n",
    "last = ext_y_pred[-60:] # This the temperatures predicted in the last 5 years\n",
    "\n",
    "for i in range(24):\n",
    "    last = last[None, :]\n",
    "    next_month = regressor.predict(last)\n",
    "    ext_y_pred = np.append(ext_y_pred, next_month)\n",
    "    last = ext_y_pred[-60:]\n",
    "\n",
    "\n",
    "y_pred_inv_scaled = sc.inverse_transform(ext_y_pred)\n",
    "actuals = sc.inverse_transform(y_test)       \n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.ylabel(\"Avg Temperature\")\n",
    "plt.plot(actuals, label=\"Actuals temps\")\n",
    "plt.plot(y_pred_inv_scaled, label=\"Predicted\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(np.arange(len(y_pred_inv_scaled))[::6], pd.date_range(start='2001-01-01', periods=len(y_pred_inv_scaled), freq = '6MS').strftime('%b %Y'), rotation=\"vertical\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that the one-to-many linear regressor can capture the seasonality of the data too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What's the average temperature predicted for June 2017? Hint index 180 = Jan 2016\n",
    "\n",
    "y_pred_inv_scaled[180+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using FF Neural Netowrk\n",
    "\n",
    "We will now try to predict temperatures using a Feed-Forward Neural Network. This network will of course have 60 input nodes, some hidden layers with relu activation functions, and one output node for the predicted temperature.\n",
    "FF Neural Networks can virtually solve any problem thanks to the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem), that states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions. You can find a more detailed explanation in this [blog post](https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "NN_model = Sequential()\n",
    "\n",
    "# The Input Layer :\n",
    "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "NN_model.add(Dense(256, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "# The Output Layer :\n",
    "NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))\n",
    "\n",
    "# Compile the network :\n",
    "adam = optimizers.Adam(learning_rate=0.0005, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "NN_model.compile(loss='mse', optimizer=adam)\n",
    "NN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.callbacks.callbacks import ModelCheckpoint\n",
    "#checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "#checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "#callbacks_list = [checkpoint]\n",
    "#NN_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test), callbacks=callbacks_list)\n",
    "NN_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the Validation loss \n",
    "\n",
    "plt.plot(NN_model.history.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = NN_model.predict(X_test)\n",
    "y_pred_inv_scaled = sc.inverse_transform(y_pred)\n",
    "actuals = sc.inverse_transform(y_test)\n",
    "actuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE: %f'%mean_squared_error(actuals, y_pred_inv_scaled))\n",
    "print('RMSE: %f'%np.sqrt(mean_squared_error(actuals, y_pred_inv_scaled)))\n",
    "print('R-Squared: %f'%r2_score(actuals, y_pred_inv_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that this model performs a bit worse than the Linear Regression above. We will now try with LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
    "\n",
    "Preparing the 3D input vector for the LSTM. \n",
    "\n",
    "Remember, the input vector for LSTM is 3D array: (num_samples, num_time_steps, num_features). In this case we have num of time steps = 50 and num_features = 1 (Extending the same analogy we saw in the previous point, that I found very useful in understanding why the input shape has to be like this — lets say, we have 50 words in one sentence and each word is represented by a word vector. So we need 50 time steps to go through each word vector in the sentence as an input to the LSTM at each time step. There is one sentence per observation and hence num_features = 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepring rank 3 input tensors\n",
    "\n",
    "train_X = train.iloc[:,:-1]\n",
    "train_y = train.iloc[:,-1]\n",
    "test_X = test.iloc[:,:-1]\n",
    "test_y = test.iloc[:,-1]\n",
    "train_X = train_X.values\n",
    "train_y = train_y.values\n",
    "test_X = test_X.values\n",
    "test_y = test_y.values\n",
    "\n",
    "train_X = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "test_X = test_X.reshape(test_X.shape[0],test_X.shape[1],1)\n",
    "\n",
    "train_X.shape, train_y.shape, test_X.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape[1], train_X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative LSTM\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(input_shape = (train_X.shape[1], train_X.shape[2]), output_dim=train_X.shape[1], return_sequences = True))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(LSTM(256))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.add(Activation(\"linear\"))\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "lstm_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a91d42014ea939208b55a93fc68a771542bfb14d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "lstm_model.fit(train_X,train_y, batch_size=32, nb_epoch=18, shuffle=False, validation_data=(test_X, test_y))\n",
    "\n",
    "print(\"> Compilation Time : \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lstm_model.history.history['val_loss'])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2dbd4166c0c7dd3506c8a16fa67a286a9ca63e18"
   },
   "outputs": [],
   "source": [
    "# Doing a prediction on all the test data at once\n",
    "y_pred = lstm_model.predict(test_X)\n",
    "y_pred_inv_scaled = sc.inverse_transform(y_pred)\n",
    "actuals = sc.inverse_transform(test_y)\n",
    "actuals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MSE: %f'%mean_squared_error(actuals, y_pred_inv_scaled))\n",
    "print('RMSE: %f'%np.sqrt(mean_squared_error(actuals, y_pred_inv_scaled)))\n",
    "print('R-Squared: %f'%r2_score(actuals, y_pred_inv_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances are similar to the FF Neural Net, we are confident that with some hyperparameter tuning we can get similar MSE to the Linear Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_y_pred = y_pred.copy()\n",
    "last = ext_y_pred[-60:].reshape(-1,1) # This is the value predicted for 2016\n",
    "\n",
    "for i in range(24):\n",
    "    last = last[None, :] \n",
    "    next_month = lstm_model.predict(last)\n",
    "    ext_y_pred=np.append(ext_y_pred, next_month)\n",
    "    last = ext_y_pred[-60:].reshape(-1,1)\n",
    "\n",
    "y_pred_inv_scaled = sc.inverse_transform(ext_y_pred)\n",
    "actuals = sc.inverse_transform(y_test)       \n",
    "\n",
    "plt.figure(figsize=(20,6))\n",
    "plt.ylabel(\"Avg Temperature\")\n",
    "plt.plot(actuals, label=\"Actuals temps\")\n",
    "plt.plot(y_pred_inv_scaled, label=\"Predicted\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.xticks(np.arange(len(y_pred_inv_scaled))[::6], pd.date_range(start='2001-01-01', periods=len(y_pred_inv_scaled), freq = '6MS').strftime('%b %Y'), rotation=\"vertical\")\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model predicts according to seasonality correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_test_window_preds(model, n_future_preds):\n",
    "\n",
    "    ''' \n",
    "        model - The trained model we are using to predict\n",
    "        n_future_preds - Represents the number of future predictions we want to make\n",
    "                         This coincides with the number of windows that we will move forward\n",
    "                         on the test data\n",
    "    '''\n",
    "    preds_moving = []                                    # Use this to store the prediction made on each test window\n",
    "    moving_test_window = [test_X[0,:].tolist()]          # Creating the first test window\n",
    "    moving_test_window = np.array(moving_test_window)    # Making it an numpy array\n",
    "    \n",
    "    for i in range(n_future_preds):\n",
    "        preds_one_step = model.predict(moving_test_window) # Note that this is already a scaled prediction so no need to rescale this\n",
    "        preds_moving.append(preds_one_step[0,0]) # get the value from the numpy 2D array and append to predictions\n",
    "        preds_one_step = preds_one_step.reshape(1,1,1) # Reshaping the prediction to 3D array for concatenation with moving test window\n",
    "        moving_test_window = np.concatenate((moving_test_window[:,1:,:], preds_one_step), axis=1) # This is the new moving test window, where the first element from the window has been removed and the prediction  has been appended to the end\n",
    "        \n",
    "    preds_moving = sc.inverse_transform(preds_moving)\n",
    "    \n",
    "    return preds_moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_moving = moving_test_window_preds(lstm_model, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actuals)\n",
    "plt.plot(preds_moving)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_moving = moving_test_window_preds(lstm_model, 50)\n",
    "plt.plot(actuals)\n",
    "plt.plot(preds_moving)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible exerrcises:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you show climate change impact in SA vs (Denmark?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you predict the temperature of July 2020 in Cape Town?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does to relate to some factors that can be changed like CO2 emissions, food production, natural disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate Change and Natural Disasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural disasters\n",
    "# Natural diasaster data is taken from https://ourworldindata.org/natural-disasters (data published by EMDAT (2019):\n",
    "# OFDA/CRED International Disaster Database, Université catholique de Louvain – Brussels – Belgium)\n",
    "nat_disaster_df = pd.read_csv('data/number-of-natural-disaster-events.csv')\n",
    "\n",
    "nat_disaster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Code' column\n",
    "nat_disaster_df.drop(['Code'], axis = 1, inplace = True)\n",
    "\n",
    "# Check the different types of 'Entity' values\n",
    "nat_disaster_df['Entity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the data according to the disaster categories.\n",
    "nat_disaster_df = nat_disaster_df.pivot(index = 'Year', columns = 'Entity', values = 'Number of reported natural disasters (reported disasters)')\n",
    "nat_disaster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'Impact' column\n",
    "nat_disaster_df.drop(['Impact'], axis = 1, inplace = True)\n",
    "\n",
    "# Handle missing values and rename columns\n",
    "nat_disaster_df.fillna(value = 0, inplace = True)\n",
    "nat_disaster_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the different types of Natural disaster to see if they increased over time\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15,12))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title('Global occurrences of Flood 1950-2018', fontsize = 10)\n",
    "nat_disaster_df.drop(['All natural disasters'], axis = 1).loc[1950:, 'Flood'].plot.bar()\n",
    "plt.ylabel('Occurrences', fontsize = 15)\n",
    "custom_ticks = np.arange(0, 2020-1950, 5)\n",
    "custom_labels = np.arange(1950, 2020, 5)\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title('Global occurrences of Drought 1950-2018', fontsize = 10)\n",
    "nat_disaster_df.drop(['All natural disasters'], axis = 1).loc[1950:, 'Drought'].plot.bar()\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title('Global occurrences of Wildfire 1950-2018', fontsize = 10)\n",
    "nat_disaster_df.drop(['All natural disasters'], axis = 1).loc[1950:, 'Wildfire'].plot.bar()\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title('Global occurrences of Earthquake 1950-2018', fontsize = 10)\n",
    "nat_disaster_df.drop(['All natural disasters'], axis = 1).loc[1950:, 'Earthquake'].plot.bar()\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title('Global occurrences of Extreme temperature 1950-2018', fontsize = 10)\n",
    "nat_disaster_df.drop(['All natural disasters'], axis = 1).loc[1950:, 'Extreme temperature'].plot.bar()\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title('Global occurrences of Extreme weather 1950-2018', fontsize = 10)\n",
    "nat_disaster_df.drop(['All natural disasters'], axis = 1).loc[1950:, 'Extreme weather'].plot.bar()\n",
    "plt.xticks(custom_ticks, custom_labels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some of these plots might seem inconclusive, the data on flood and extreme weather shows a marked increase of these phenomenon in the last years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will plot natural disaster occurrences compared with the temperatures anomalies phenomenon we explored above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all natural disasters occurrences and temperature anomaly for comparison\n",
    "fig, ax = plt.subplots(figsize = (14, 8))\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "#line1 = ax.plot(nat_disaster_df.loc[:2015, 'All natural disasters'], '-ro', markersize = 4, label = 'Natural disasters')\n",
    "#line2 = ax2.plot(global_mean[global_mean.year>=1900].set_index('year').TemperatureAnomaly, 'b-', label = 'Temperature Anomaly')\n",
    "\n",
    "line1 = ax.plot(nat_disaster_df.loc[:2015, 'All natural disasters'], '-r', label = 'Natural Disasters')\n",
    "line2 = ax2.plot(global_mean[global_mean.year>=1900].set_index('year').TemperatureAnomaly, label = 'Temperature Anomaly')\n",
    "\n",
    "\n",
    "\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "\n",
    "plt.title('Natural disasters  and Temperature Anomaly 1900-2015', fontsize = 15)\n",
    "ax.set_xlabel('Year', fontsize = 15)\n",
    "ax.set_ylabel('Disaster Occurrences', fontsize = 15)\n",
    "ax2.set_ylabel('Temperature anomaly (Celsius)', fontsize = 15)\n",
    "ax.legend(lines, labels, loc = 0, prop = {'size': 12})\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly shows a correlation between the increase in temperatures and natural disasters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the economic damage dataset and store it in a dataframe\n",
    "econ_dmg_df = pd.read_csv('data/economic-damage-from-natural-disasters.csv')\n",
    "\n",
    "\n",
    "econ_dmg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible questions:\n",
    "\n",
    "# Can you show a correlation between Climate change and economic damage from natural disasters?\n",
    "# Can you find correlations between the different types of natural disasters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
